{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLJZ8UHJawGb"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSfZRISXyaoF"
   },
   "outputs": [],
   "source": [
    "%pip install -q grad-cam\n",
    "\n",
    "from utils import imshow, imagenette_outputs, multiple_c_o_m, shift\n",
    "from ImagenetteDataset import ImagenetteDataset\n",
    "from load_model import load_model\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import torchvision, torch, torchvision.transforms as T\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH7FsIOIbDWL"
   },
   "source": [
    "Configure Size of Imagenette Pictures and PyTorch Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue_vBa_JZ3PS"
   },
   "outputs": [],
   "source": [
    "#160 uses ~8GB RAM, 320 uses ~24GB RAM, Fullsize not tested\n",
    "size = 160\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#   device = torch.device(\"mps\")\n",
    "#   import os\n",
    "#   os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"]=\"1\"\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoqH1NJpbJoX"
   },
   "source": [
    "Download and unpack images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or3BVyJpZ3PT"
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile(f'imagenette2-{size}.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-{size}.tgz\n",
    "    !tar -xf imagenette2-{size}.tgz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model and target Layers for GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, target_layers = load_model('mobilenet', norm_layer=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLu5lm7dZ3PT"
   },
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "trainset = ImagenetteDataset(size, should_normalize=False)\n",
    "valset = ImagenetteDataset(size, validation=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = bs, shuffle = True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = bs, shuffle = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first Batch for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkD91UmIMWBF",
    "outputId": "2b37a821-dc6b-4536-fd89-96e7439c352a"
   },
   "outputs": [],
   "source": [
    "data_batch, labels_batch = next(iter(trainloader))\n",
    "print(data_batch.size())\n",
    "print(labels_batch.size())\n",
    "out = torchvision.utils.make_grid(data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict First Batch with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ik0SKRFNC2G",
    "outputId": "ea3a0fd3-95b8-4f52-c4d0-c10c9a96f6fe"
   },
   "outputs": [],
   "source": [
    "class_names = trainset.classes\n",
    "print(class_names)\n",
    "outputs = model(data_batch.to(device))\n",
    "print(outputs.shape)\n",
    "preds = imagenette_outputs(outputs)\n",
    "print(labels_batch)\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "EmmGeFAKNhtq",
    "outputId": "2c93d222-0194-47f7-a1fd-040c9c0347b3"
   },
   "outputs": [],
   "source": [
    "imshow(out, denorm=False)#, title=[class_names[x] for x in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Inference on whole trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljCAwTCGVu5U"
   },
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "num_correct = 0\n",
    "with torch.no_grad():\n",
    "  loop = tqdm(trainloader)\n",
    "  for idx, (data, labels) in enumerate(loop):\n",
    "    outputs = model(data.to(device))\n",
    "    preds = imagenette_outputs(outputs)\n",
    "    all_predictions.extend(preds)\n",
    "    corrects = torch.sum(preds == labels.to(device))\n",
    "    num_correct += corrects\n",
    "    loop.set_description(f\"Processing batch {idx+1}\")\n",
    "    loop.set_postfix(current_accuracy = num_correct.double().item()/(len(labels)*(idx+1)))\n",
    "    #print(f\"Done with batch of size {(len(labels))}\")\n",
    "pred = torch.stack(all_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL_oUx2ijnEO",
    "outputId": "381e7fbd-1e26-4e21-bfdb-f8fc7133f7ed"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.4f}\".format(num_correct.double()/len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WECFaYIHXQw_",
    "outputId": "011daa5d-1384-411b-f3c1-1055384557b6"
   },
   "outputs": [],
   "source": [
    "print(pred.size())\n",
    "print(pred[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Grad-Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "rC5xZWeikNrA",
    "outputId": "b7a157a6-b161-433e-e1e4-7c87c104ba56"
   },
   "outputs": [],
   "source": [
    "\n",
    "cam = GradCAM(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available())\n",
    "#with torch.no_grad():\n",
    "# 3:23 for 320px and resnet50\n",
    "# 2:33 for 320px and mobilenetv3\n",
    "dirname=f'output_{size}'\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "loop = tqdm(trainloader)\n",
    "examples= []\n",
    "for batch, (data, labels) in enumerate(loop):\n",
    "  #make sure we have even number of samples\n",
    "  if len(labels) % 2 != 0:\n",
    "    data = data[:-1,:,:,:]\n",
    "  #Stop after 10 batches to make it quicker\n",
    "  if batch == 3:\n",
    "    break\n",
    "  grayscale_cam = cam(input_tensor=data, targets=None)\n",
    "\n",
    "  \n",
    "  threshold = np.quantile(grayscale_cam.flatten(), .85)\n",
    "  b_mask = np.where(grayscale_cam>threshold, np.ones_like(grayscale_cam), np.zeros_like(grayscale_cam))\n",
    "  c_o_m = multiple_c_o_m(b_mask)\n",
    "\n",
    "  imgs_base, imgs_attack = np.array_split(data.cpu().numpy(), 2, axis=0)\n",
    "  masks_base, masks_attack = np.array_split(b_mask, 2, axis=0)\n",
    "  c_o_m_base, c_o_m_attack = np.array_split(c_o_m, 2, axis=0)\n",
    "  offsets = (c_o_m_base - c_o_m_attack).astype(int)\n",
    "  for base_img, attack_img, base_mask, offset in zip(imgs_base, imgs_attack, masks_base, offsets):\n",
    "    invariance_adv = np.where(base_mask==True, shift(attack_img, offset), base_img)\n",
    "    all_examples.append(invariance_adv[:,:,:])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Pictures to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img in enumerate(examples):\n",
    "  img = (img*255).astype(np.uint8)\n",
    "  img = Image.fromarray(np.uint8(img.transpose(1,2,0)))\n",
    "  img.save(f\"/content/drive/MyDrive/adv_examples_320/{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of the Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, xarr = plt.subplots(3,3, figsize=(15,15))\n",
    "xarr.flatten()\n",
    "for idx, ax in enumerate(xarr.flatten()):\n",
    "  ax.imshow(batch_examples[idx].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 4\n",
    "com_b = c_o_m_base[idx]\n",
    "com_a = c_o_m_attack[idx]\n",
    "offset = offsets[idx]\n",
    "base_image = imgs_base[idx]\n",
    "attack_image = imgs_attack[idx]\n",
    "print (base_image.shape)\n",
    "print (attack_image.shape)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 20))\n",
    "ax = ax.flatten()\n",
    "ax[0].imshow(base_image.transpose(1,2,0))\n",
    "ax[0].scatter(com_b[0], com_b[1], s=size, c='C0', marker='+')\n",
    "ax[1].imshow(attack_image.transpose(1,2,0))\n",
    "ax[1].scatter(com_a[0], com_a[1], s=size, c='C1', marker='+')\n",
    "ax[1].scatter(com_b[0], com_b[1], s=size, c='C0', marker='+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attack_image.shape)\n",
    "shifted = shift(attack_image, offset)\n",
    "#print(offset[::-1])\n",
    "print (shifted.shape)\n",
    "plt.imshow(shifted.transpose(1,2,0))\n",
    "\n",
    "# print(com_b, com_a, offset)\n",
    "# attack_image_cropped = attack_image[:,39:,11:]\n",
    "# #plt.imshow(attack_image_cropped.transpose(1,2,0))\n",
    "# print(attack_image_cropped.shape)\n",
    "# empty = np.zeros_like(attack_image)\n",
    "# empty[:,0:121,0:149] = attack_image_cropped\n",
    "# print(empty.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invariance_adv = np.where(masks_base[0]==True, shifted, base_image)\n",
    "plt.imshow(invariance_adv.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import shift\n",
    "print(offsets[11])\n",
    "print(masked_base[11].shape)\n",
    "print(offsets[0,0])\n",
    "print (offsets[:,0])\n",
    "one_image = imgs_attack[:,:,offsets[:,0]:,offsets[:,1]:]\n",
    "\n",
    "#shifted = shift(masked_base[11], offsets[11], cval=0)\n",
    "plt.imshow(attack_patches[4].transpose(1,2,0))\n",
    "#plt.imshow(masked_base[11].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalized.shape)\n",
    "fig, ax = plt.subplots(2, 4, figsize=(20, 20))\n",
    "ax = ax.flatten()\n",
    "for i in range(8):\n",
    "  idx = random.randint(0, len(masked_images)-1)\n",
    "  ax[i].imshow(normalized[idx].transpose(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_o_m = multiple_c_o_m(masked_images)\n",
    "\n",
    "print(c_o_m.shape)\n",
    "#c_o_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 4, figsize=(20, 20))\n",
    "ax = ax.flatten()\n",
    "for i in range(8):\n",
    "  idx = random.randint(0, len(masked_images)-1)\n",
    "  ax[i].imshow(masked_images[idx].transpose(1,2,0))\n",
    "  ax[i].scatter(c_o_m[idx][0], c_o_m[idx][1], s=size, c='C0', marker='+')\n",
    "  print(idx, c_o_m[idx])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not needed\n",
    "threshold = np.quantile(gradcam_hm.flatten(), .85)\n",
    "b_mask = np.where(gradcam_hm>threshold, np.ones_like(gradcam_hm), np.zeros_like(gradcam_hm))\n",
    "print (b_mask.shape)\n",
    "img_batch = next(iter(trainloader))[0]\n",
    "idx = 4\n",
    "plt.imshow((b_mask[idx]*img_batch[idx].detach().cpu().numpy()).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainability with Pytorch Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5D3WX7dHUJH",
    "outputId": "4659f143-d177-44af-8b8c-2d9f49dce043"
   },
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/pytorch/captum.git\n",
    "\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "Ed38iqlIMJvC",
    "outputId": "da4dbedd-0c83-4ea5-a520-985b97d5bb0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "ig = IntegratedGradients(model)\n",
    "data, labels = next(iter(trainloader))\n",
    "idx = 4\n",
    "input = data[idx].unsqueeze(0).to(device)\n",
    "label = labels[idx].to(device)\n",
    "#print (data[0].size())\n",
    "attributions = ig.attribute(input, target=label, n_steps=100)\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "_ = viz.visualize_image_attr(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             np.transpose(data[idx].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             method='heat_map',\n",
    "                             cmap=default_cmap,\n",
    "                             show_colorbar=True,\n",
    "                             sign='positive',\n",
    "                             outlier_perc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "EE8JU9NXNCu9",
    "outputId": "6f291f3b-a029-4990-c88a-4c22155d1a3b"
   },
   "outputs": [],
   "source": [
    "imshow(data[idx], denorm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Tunnel for Smooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "3EhEhJegSYjB",
    "outputId": "757aaa6e-01bc-4f54-8ee0-efd8d54edeb2"
   },
   "outputs": [],
   "source": [
    "# nt_samples <= 7 for 15GB VRAM \n",
    "noise_tunnel = NoiseTunnel(ig)\n",
    "\n",
    "attributions_ig_nt = noise_tunnel.attribute(input, nt_samples=20, nt_type='smoothgrad_sq', target=label)\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      np.transpose(data[idx].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      [\"original_image\", \"heat_map\"],\n",
    "                                      [\"all\", \"positive\"],\n",
    "                                      cmap=default_cmap,\n",
    "                                      show_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ4JAV18UVBF"
   },
   "outputs": [],
   "source": [
    "#plt.imshow(show_cam_on_image(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    " #                                     np.transpose(data[idx].squeeze().cpu().detach().numpy(), (1,2,0)), use_rgb=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('/Users/dibse/SynologyDrive/Bachelorarbeit/Importance_based_Adversarial_Examples/imagenette2-160/train/n01440764/ILSVRC2012_val_00000293.JPEG')\n",
    "grayscale_cam = cam(input_tensor=torch.tensor(image).unsqueeze(0), targets=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
