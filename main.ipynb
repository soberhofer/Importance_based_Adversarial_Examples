{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLJZ8UHJawGb"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "# If in Colab, we need to pull utilities from github\n",
    "if IN_COLAB:\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/load_model.py\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/utils.py\n",
    "  !wget https://raw.githubusercontent.com/soberhofer/Importance_based_Adversarial_Examples/main/ImagenetteDataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSfZRISXyaoF",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q grad-cam\n",
    "\n",
    "from utils import imshow, imagenette_outputs, multiple_c_o_m, shift\n",
    "from ImagenetteDataset import ImagenetteDataset\n",
    "from load_model import load_model\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import torchvision, torch, torchvision.transforms as T\n",
    "from pytorch_grad_cam import GradCAM, HiResCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, EigenGradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import center_of_mass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bH7FsIOIbDWL"
   },
   "source": [
    "Configure Size of Imagenette Pictures and PyTorch Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ue_vBa_JZ3PS"
   },
   "outputs": [],
   "source": [
    "#160 uses ~8GB RAM, 320 uses ~24GB RAM, Fullsize not tested\n",
    "size = 160\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "# elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#    %env PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "else:\n",
    "  device = \"cpu\"\n",
    "\n",
    "print(device)\n",
    "#EigenGradCAM ScoreCAM seems not to work with mps\n",
    "# AblationCAM is funky\n",
    "cams = [XGradCAM]\n",
    "#cams = [EigenCAM, XGradCAM, GradCAM, HiResCAM, GradCAMPlusPlus]\n",
    "#cams = [EigenCAM]\n",
    "#%env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoqH1NJpbJoX"
   },
   "source": [
    "Download and unpack images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "or3BVyJpZ3PT",
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if size in [160, 320]:\n",
    "  #Download resized images\n",
    "  if not os.path.isfile(f'imagenette2-{size}.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-{size}.tgz\n",
    "    !tar -xf imagenette2-{size}.tgz\n",
    "elif os.path.isdir(f\"imagenette2-{size}\"):\n",
    "    print(\"Data is present, continuing\")\n",
    "else:\n",
    "  #Download original images\n",
    "  print(\"Downloading originals and resizing\")\n",
    "  if not os.path.isfile(f'imagenette2.tgz'):\n",
    "    !wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n",
    "    !tar -xf imagenette2.tgz\n",
    "    # Downscale to chosen size\n",
    "    folder_dir = f\"imagenette2-{size}\"\n",
    "    os.rename(\"imagenette2\",folder_dir)\n",
    "    for dataset in [\"train\",\"val\"]:\n",
    "      for classes in os.listdir(f\"{folder_dir}/{dataset}\"):\n",
    "        for image in os.listdir(f\"{folder_dir}/{dataset}/{classes}\"):\n",
    "          image_path = f\"{folder_dir}/{dataset}/{classes}/{image}\"\n",
    "          img = Image.open(image_path)\n",
    "          img.thumbnail((size,size))\n",
    "          os.remove(image_path)\n",
    "          img.save(image_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model and target Layers for GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "model, target_layers = load_model('mobilenet', norm_layer=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build our DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLu5lm7dZ3PT",
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "bs = 32\n",
    "\n",
    "trainset = ImagenetteDataset(size, should_normalize=False)\n",
    "valset = ImagenetteDataset(size, should_normalize=False, validation=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = bs, shuffle = True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size = bs, shuffle = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get first Batch for Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkD91UmIMWBF",
    "outputId": "2b37a821-dc6b-4536-fd89-96e7439c352a"
   },
   "outputs": [],
   "source": [
    "data_batch, labels_batch = next(iter(trainloader))\n",
    "print(data_batch.size())\n",
    "print(labels_batch.size())\n",
    "out = torchvision.utils.make_grid(data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict First Batch with our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ik0SKRFNC2G",
    "outputId": "ea3a0fd3-95b8-4f52-c4d0-c10c9a96f6fe"
   },
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "class_names = trainset.classes\n",
    "print(class_names)\n",
    "outputs = model(data_batch.to(device))\n",
    "print(outputs.shape)\n",
    "preds = imagenette_outputs(outputs)\n",
    "print(labels_batch)\n",
    "#print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "EmmGeFAKNhtq",
    "outputId": "2c93d222-0194-47f7-a1fd-040c9c0347b3"
   },
   "outputs": [],
   "source": [
    "imshow(out, denorm=False)#, title=[class_names[x] for x in preds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Inference on whole trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljCAwTCGVu5U"
   },
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "num_correct = 0\n",
    "with torch.no_grad():\n",
    "  loop = tqdm(trainloader)\n",
    "  for idx, (data, labels) in enumerate(loop):\n",
    "    outputs = model(data.to(device))\n",
    "    preds = imagenette_outputs(outputs)\n",
    "    all_predictions.extend(preds)\n",
    "    corrects = torch.sum(preds == labels.to(device))\n",
    "    num_correct += corrects\n",
    "    loop.set_description(f\"Processing batch {idx+1}\")\n",
    "    loop.set_postfix(current_accuracy = num_correct.double().item()/(len(labels)*(idx+1)))\n",
    "    #print(f\"Done with batch of size {(len(labels))}\")\n",
    "pred = torch.stack(all_predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pL_oUx2ijnEO",
    "outputId": "381e7fbd-1e26-4e21-bfdb-f8fc7133f7ed"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {:.4f}\".format(num_correct.double()/len(trainset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WECFaYIHXQw_",
    "outputId": "011daa5d-1384-411b-f3c1-1055384557b6"
   },
   "outputs": [],
   "source": [
    "print(pred.size())\n",
    "print(pred[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "rC5xZWeikNrA",
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "outputId": "b7a157a6-b161-433e-e1e4-7c87c104ba56"
   },
   "outputs": [],
   "source": [
    "#Iterate over all cams\n",
    "for ourcam in cams:\n",
    "  folder = f\"./adv_examples_{ourcam.__name__}_{size}/\"\n",
    "  if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "  cam = ourcam(model=model, target_layers=target_layers, use_cuda=torch.cuda.is_available(), use_mps=True)\n",
    "  \n",
    "  torch.manual_seed(42)\n",
    "  # To avoid bias, we only use images which have been used as test set during training\n",
    "  loop = tqdm(valloader)\n",
    "  examples = []\n",
    "  found = 0\n",
    "  thirdlabel = 0\n",
    "  same = 0\n",
    "  invalid = 0\n",
    "  bad_ex = 0\n",
    "    \n",
    "  for batch, (data, labels) in enumerate(loop):\n",
    "    #stop after 10% of the dataset\n",
    "    #if batch > len(valloader)//5:\n",
    "    #  break\n",
    "    #make sure we have even number of samples, if not, remove the last one. Use even block size to avoid this\n",
    "    if len(labels) % 2 != 0:\n",
    "      data = data[:-1,:,:,:]\n",
    "      labels = labels[:-1]\n",
    "\n",
    "    # Sort the batch so that the base and attack image do not have the same label\n",
    "    # we try it for bs^2 times and then stop, some batches are not sortable in this way\n",
    "    # we should get almost all of them sorted nicely though\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "    correct = False\n",
    "    tries = 0\n",
    "    loop.set_description(f\"Sorting batch...\")\n",
    "    while (not correct and tries < bs**2):\n",
    "      swapped = False\n",
    "      for idx, img in enumerate(data):\n",
    "        if idx >= len(labels)/2:\n",
    "          break\n",
    "        if labels[idx] == labels[idx+int((len(labels)/2))]:\n",
    "          labels[idx], labels[idx+1] = labels[idx+1], labels[idx]\n",
    "          data[idx], data[idx+1] = data[idx+1], data[idx]\n",
    "          swapped = True\n",
    "      if not swapped:\n",
    "        correct = True\n",
    "      tries += 1\n",
    "    # get the CAMs for the batch\n",
    "    grayscale_cam = cam(input_tensor=data, targets=None)\n",
    "    cams_base, cams_attack = np.array_split(grayscale_cam, 2, axis=0)\n",
    "    imgs_base, imgs_attack = np.array_split(data.cpu().numpy(), 2, axis=0)\n",
    "    labels_base, labels_attack = np.array_split(labels.cpu().numpy(), 2, axis=0)\n",
    "    #iterate over each batch\n",
    "    for base_img, attack_img, base_cam, attack_cam, base_label, attack_label in zip(imgs_base, imgs_attack, cams_base, cams_attack, labels_base, labels_attack):\n",
    "      # ignore pairs with same label (should not happen too often now)\n",
    "      if (attack_label == base_label):\n",
    "        same += 1\n",
    "        continue\n",
    "      #start with a 1% mask\n",
    "      current_threshold = 0.99\n",
    "      \n",
    "      # Look for the adversarial Example\n",
    "      while True:\n",
    "        loop.set_description(f\"Found: {found}, 3rdlabel: {thirdlabel} same label: {same}, invalid: {invalid}, bad_ex: {bad_ex}, using {ourcam.__name__}\")\n",
    "        base_threshold = np.quantile(base_cam.flatten(), current_threshold)\n",
    "        attack_threshold = np.quantile(attack_cam.flatten(), current_threshold)\n",
    "        base_mask = np.where(base_cam>base_threshold, np.ones_like(base_cam), np.zeros_like(base_cam))\n",
    "        attack_mask = np.where(attack_cam>attack_threshold, np.ones_like(attack_cam), np.zeros_like(attack_cam))\n",
    "        c_o_m_base = np.array(center_of_mass(base_mask))\n",
    "        c_o_m_attack = np.array(center_of_mass(attack_mask))\n",
    "        offset = c_o_m_base - c_o_m_attack\n",
    "\n",
    "        # Remember the last image we produced, in case this is the adversarial example\n",
    "        if 'invariance_adv' in locals():\n",
    "          last_img = invariance_adv.copy()\n",
    "\n",
    "        #Produce the example\n",
    "        invariance_adv = np.where(base_mask==True, shift(attack_img, offset), base_img)\n",
    "\n",
    "        #Check output of Model\n",
    "        output = imagenette_outputs(model(torch.from_numpy(invariance_adv).unsqueeze(0).to(device)))\n",
    "\n",
    "        \n",
    "        if output.item() == base_label:\n",
    "          # threshold <= 0.01 means we have a mask of 99% -> we can't find an adversarial example\n",
    "          if current_threshold <= 0.01:\n",
    "            invalid +=1\n",
    "            break\n",
    "          #Model still predicts base label -> make mask bigger\n",
    "          current_threshold -= 0.01\n",
    "        \n",
    "        #elif output.item() == attack_label:\n",
    "        #  #We found the example. Write it to disk\n",
    "        #  found += 1\n",
    "        #  img = Image.fromarray((last_img*255).astype(np.uint8).transpose(1,2,0))\n",
    "        #  # Format of image name: base_label_attack_label_intermediate_label_threshold.jpg\n",
    "        #  #img.save(f\"/content/drive/MyDrive/adv_examples_{size}/{base_label}_{attack_label}_{current_threshold:.2f}.jpg\")\n",
    "        #  examples.append((last_img, base_label, attack_label, current_threshold))\n",
    "        #  break\n",
    "        else:\n",
    "          # threshold >= 0.99 means we have a mask of 1% and the model already flips label. We can't find an adversarial example\n",
    "          if current_threshold >= 0.99:\n",
    "            invalid +=1\n",
    "            break\n",
    "          #model flips early, we look for a better example\n",
    "          if current_threshold >= 0.3:\n",
    "            bad_ex += 1\n",
    "            break\n",
    "          #We found the example. Write it to disk\n",
    "          img = Image.fromarray((last_img*255).astype(np.uint8).transpose(1,2,0))\n",
    "          #Format of image name: base_label_attack_label_intermediate_label_threshold.jpg\n",
    "\n",
    "          img.save(f\"{folder}/{base_label}_{attack_label}_{output.item()}_{current_threshold:.2f}.jpg\")\n",
    "          examples.append((last_img, base_label, attack_label, output.item(), current_threshold))\n",
    "          if output.item() != attack_label:\n",
    "            thirdlabel += 1\n",
    "          else:\n",
    "            found += 1\n",
    "          break\n",
    "  with open(f\"{folder}/results.txt\", \"w\") as f:\n",
    "    f.write(f\"Found: {found}, 3rdlabel: {thirdlabel} same label: {same}, invalid: {invalid}, bad: {bad_ex} using {ourcam.__name__} and {size}x{size} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Check if outputs are correct\n",
    "for (img, base_label,attack_label,_,threshold) in examples:\n",
    "  output = imagenette_outputs(model(torch.from_numpy(img).unsqueeze(0).to(device)))\n",
    "  if output.item() != base_label:\n",
    "    print(f\"Wrong output for {base_label}_{attack_label}: {output.item()} with {threshold:.2f}\")\n",
    "    #print(img)\n",
    "    #plt.imshow(img.transpose(1,2,0))\n",
    "    #imshow(img, denorm=True)\n",
    "    #break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "##### XGradCam valset 320px \n",
    "Found: 135, 3rdlabel: 26 same label: 18, invalid: 109, bad_ex: 1673, cutoff: 0.3,  Median 0.25, best: 0.13. Time 38:53"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "examples.sort(key=lambda x: x[4], reverse=False)\n",
    "#examples.sort(key=lambda x: (x[4]-0.5)**2, reverse=True)\n",
    "idx = 0\n",
    "#for idx in range(30):\n",
    "#  print(f\"{examples[idx][4]:.2f}\")\n",
    "print(f\"{examples[idx][4]:.2f}, {imagenette_labels[examples[idx][1]]}, {imagenette_labels[examples[idx][2]]}, {imagenette_labels[examples[idx][3]]}\")\n",
    "#plt.imshow(examples[idx][0].transpose(1,2,0))\n",
    "thresholds = [x[4] for x in examples]\n",
    "#print median\n",
    "print(f\"Median: {np.median(thresholds):.2f}\")\n",
    "plt.hist(thresholds);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export Pictures to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, img in enumerate(examples):\n",
    "#   img = Image.fromarray((img*255).astype(np.uint8).transpose(1,2,0))\n",
    "#   img.save(f\"/content/drive/MyDrive/adv_examples_320/{idx}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some of the Pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# f, xarr = plt.subplots(2,2, figsize=(15,15))\n",
    "# xarr.flatten()\n",
    "# for idx, ax in enumerate(xarr.flatten()):\n",
    "#   ax.imshow(examples[idx][0].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 4\n",
    "# com_b = c_o_m_base[idx]\n",
    "# com_a = c_o_m_attack[idx]\n",
    "# offset = offsets[idx]\n",
    "# base_image = imgs_base[idx]\n",
    "# attack_image = imgs_attack[idx]\n",
    "# print (base_image.shape)\n",
    "# print (attack_image.shape)\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(20, 20))\n",
    "# ax = ax.flatten()\n",
    "# ax[0].imshow(base_image.transpose(1,2,0))\n",
    "# ax[0].scatter(com_b[0], com_b[1], s=size, c='C0', marker='+')\n",
    "# ax[1].imshow(attack_image.transpose(1,2,0))\n",
    "# ax[1].scatter(com_a[0], com_a[1], s=size, c='C1', marker='+')\n",
    "# ax[1].scatter(com_b[0], com_b[1], s=size, c='C0', marker='+')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(attack_image.shape)\n",
    "# shifted = shift(attack_image, offset)\n",
    "# #print(offset[::-1])\n",
    "# print (shifted.shape)\n",
    "# plt.imshow(shifted.transpose(1,2,0))\n",
    "\n",
    "# # print(com_b, com_a, offset)\n",
    "# # attack_image_cropped = attack_image[:,39:,11:]\n",
    "# # #plt.imshow(attack_image_cropped.transpose(1,2,0))\n",
    "# # print(attack_image_cropped.shape)\n",
    "# # empty = np.zeros_like(attack_image)\n",
    "# # empty[:,0:121,0:149] = attack_image_cropped\n",
    "# # print(empty.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invariance_adv = np.where(masks_base[0]==True, shifted, base_image)\n",
    "# plt.imshow(invariance_adv.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.ndimage import shift\n",
    "# print(offsets[11])\n",
    "# print(masked_base[11].shape)\n",
    "# print(offsets[0,0])\n",
    "# print (offsets[:,0])\n",
    "# one_image = imgs_attack[:,:,offsets[:,0]:,offsets[:,1]:]\n",
    "\n",
    "# #shifted = shift(masked_base[11], offsets[11], cval=0)\n",
    "# plt.imshow(attack_patches[4].transpose(1,2,0))\n",
    "# #plt.imshow(masked_base[11].transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalized.shape)\n",
    "# fig, ax = plt.subplots(2, 4, figsize=(20, 20))\n",
    "# ax = ax.flatten()\n",
    "# for i in range(8):\n",
    "#   idx = random.randint(0, len(masked_images)-1)\n",
    "#   ax[i].imshow(normalized[idx].transpose(0,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_o_m = multiple_c_o_m(masked_images)\n",
    "\n",
    "# print(c_o_m.shape)\n",
    "# #c_o_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(2, 4, figsize=(20, 20))\n",
    "# ax = ax.flatten()\n",
    "# for i in range(8):\n",
    "#   idx = random.randint(0, len(masked_images)-1)\n",
    "#   ax[i].imshow(masked_images[idx].transpose(1,2,0))\n",
    "#   ax[i].scatter(c_o_m[idx][0], c_o_m[idx][1], s=size, c='C0', marker='+')\n",
    "#   print(idx, c_o_m[idx])\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #not needed\n",
    "# threshold = np.quantile(gradcam_hm.flatten(), .85)\n",
    "# b_mask = np.where(gradcam_hm>threshold, np.ones_like(gradcam_hm), np.zeros_like(gradcam_hm))\n",
    "# print (b_mask.shape)\n",
    "# img_batch = next(iter(trainloader))[0]\n",
    "# idx = 4\n",
    "# plt.imshow((b_mask[idx]*img_batch[idx].detach().cpu().numpy()).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explainability with Pytorch Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5D3WX7dHUJH",
    "outputId": "4659f143-d177-44af-8b8c-2d9f49dce043"
   },
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/pytorch/captum.git\n",
    "\n",
    "from captum.attr import IntegratedGradients, NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "Ed38iqlIMJvC",
    "outputId": "da4dbedd-0c83-4ea5-a520-985b97d5bb0d"
   },
   "outputs": [],
   "source": [
    "\n",
    "ig = IntegratedGradients(model)\n",
    "data, labels = next(iter(trainloader))\n",
    "idx = 4\n",
    "input = data[idx].unsqueeze(0).to(device)\n",
    "label = labels[idx].to(device)\n",
    "#print (data[0].size())\n",
    "attributions = ig.attribute(input, target=label, n_steps=100)\n",
    "\n",
    "default_cmap = LinearSegmentedColormap.from_list('custom blue',\n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#000000'),\n",
    "                                                  (1, '#000000')], N=256)\n",
    "\n",
    "_ = viz.visualize_image_attr(np.transpose(attributions.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             np.transpose(data[idx].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                             method='heat_map',\n",
    "                             cmap=default_cmap,\n",
    "                             show_colorbar=True,\n",
    "                             sign='positive',\n",
    "                             outlier_perc=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "EE8JU9NXNCu9",
    "outputId": "6f291f3b-a029-4990-c88a-4c22155d1a3b"
   },
   "outputs": [],
   "source": [
    "imshow(data[idx], denorm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Tunnel for Smooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "3EhEhJegSYjB",
    "outputId": "757aaa6e-01bc-4f54-8ee0-efd8d54edeb2"
   },
   "outputs": [],
   "source": [
    "# nt_samples <= 7 for 15GB VRAM \n",
    "noise_tunnel = NoiseTunnel(ig)\n",
    "\n",
    "attributions_ig_nt = noise_tunnel.attribute(input, nt_samples=20, nt_type='smoothgrad_sq', target=label)\n",
    "_ = viz.visualize_image_attr_multiple(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      np.transpose(data[idx].squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                      [\"original_image\", \"heat_map\"],\n",
    "                                      [\"all\", \"positive\"],\n",
    "                                      cmap=default_cmap,\n",
    "                                      show_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZ4JAV18UVBF"
   },
   "outputs": [],
   "source": [
    "#plt.imshow(show_cam_on_image(np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    " #                                     np.transpose(data[idx].squeeze().cpu().detach().numpy(), (1,2,0)), use_rgb=True))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
